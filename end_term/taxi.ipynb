{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taxi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqk6JjzZTkUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d123542-6b94-4caf-ecab-fecfcde95b9d"
      },
      "source": [
        "!pip install pretty_errors\r\n",
        "!pip install catboost\r\n",
        "!pip install lightgbm\r\n",
        "!pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretty_errors in /usr/local/lib/python3.6/dist-packages (1.2.19)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from pretty_errors) (0.4.4)\n",
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/37/bc4e0ddc30c07a96482abf1de7ed1ca54e59bba2026a33bca6d2ef286e5b/catboost-0.24.4-cp36-none-manylinux1_x86_64.whl (65.7MB)\n",
            "     |████████████████████████████████| 65.8MB 98kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.19.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.4\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (1.0.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (51.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.19.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->mlxtend) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VXqS15Vseto"
      },
      "source": [
        "# **分為import部分、資料處理部分、封裝AI模型部分、tensorflow dnn模型部分以及主程式**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXJP0FBYszFn"
      },
      "source": [
        "詳細程式碼內容可看註解說明"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGmV-0bwRuwG"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\r\n",
        "import pretty_errors\r\n",
        "from time import process_time\r\n",
        "import datetime\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from sklearn.neural_network import MLPRegressor\r\n",
        "# multi-layer Perceptron regressor\r\n",
        "from sklearn.datasets import make_regression\r\n",
        "# random regression\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "# gradient Boosting for regression\r\n",
        "from catboost import CatBoostRegressor\r\n",
        "# gradient boosting on decision trees\r\n",
        "from lightgbm import LGBMRegressor\r\n",
        "# leaf-wise gradient boosting model\r\n",
        "from xgboost import XGBRegressor\r\n",
        "# extreme gradient boosting\r\n",
        "from sklearn.svm import SVR\r\n",
        "# support vector regression\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "# Lasso\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "# Random Forest\r\n",
        "from mlxtend.regressor import StackingCVRegressor\r\n",
        "# StackingCV, no used\r\n",
        "\r\n",
        "data_path = '/content/drive/My Drive/天氣AI/end_term/'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTVfvop9sD4X"
      },
      "source": [
        "# Deal with taxi_data\r\n",
        "def taxi_data_deal():\r\n",
        "    # Create train/test dataset(data:without SalePrice, label:SalePrice)\r\n",
        "    # train_lighter has 100000 data, should enough\r\n",
        "    try:\r\n",
        "        test_data = pd.read_csv(os.path.join(data_path, 'test.csv'))\r\n",
        "        train_data = pd.read_csv(os.path.join(data_path, 'train_lighter.csv'))\r\n",
        "    except FileNotFoundError:\r\n",
        "        test_data = pd.read_csv(os.path.join(data_path, 'taxi', 'test.csv'))\r\n",
        "        train_data = pd.read_csv(os.path.join(data_path, 'taxi', 'train_lighter.csv'))\r\n",
        "    train_label = train_data['fare_amount']\r\n",
        "    train_data = train_data.drop('fare_amount', axis=1)\r\n",
        "\r\n",
        "    df = pd.concat([train_data, test_data], axis = 0)\r\n",
        "\r\n",
        "    # train_data/test_data has been checked, no any NAN value, so skip fillna\r\n",
        "    '''\r\n",
        "    NANColumns = []\r\n",
        "    i = 0\r\n",
        "    for a in train_data.isnull().sum():\r\n",
        "        # if this column has null values\r\n",
        "        if a != 0:\r\n",
        "            print(train_data.columns[i], 'loss {} values'.format(a))\r\n",
        "            NANColumns.append(train_data.columns[i])\r\n",
        "        i += 1\r\n",
        "    print()\r\n",
        "    print('train_data have {} columns'.format(i))\r\n",
        "    print('but {} columns have null values'.format(len(NANColumns)))\r\n",
        "\r\n",
        "    # Handmade classification\r\n",
        "    num_list = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\r\n",
        "                'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\r\n",
        "                'FullBath', 'HalfBath', 'Bedroom', 'Kitchen', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea',\r\n",
        "                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\r\n",
        "    str_list = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\r\n",
        "                'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual',\r\n",
        "                'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\r\n",
        "                'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\r\n",
        "                'PavedDrive', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\r\n",
        "    drop_list = ['Id', 'Alley', 'PoolQC', 'Fence', 'MiscFeature', 'MasVnrType', 'FireplaceQu']\r\n",
        "    #Id/MasVnrType useless, Alley/PoolQC/Fence/MiscFeature/FireplaceQu too many loss\r\n",
        "\r\n",
        "    # Replace NA value to mean/'X', and drop some columns\r\n",
        "    print('begin replace null values:')\r\n",
        "    print('---num gogo---')\r\n",
        "    for num_name in num_list:\r\n",
        "        print('{} ok'.format(num_name))\r\n",
        "        if num_name in NANColumns:\r\n",
        "            train_data[num_name] = train_data[num_name].replace(np.nan, np.mean(train_data[num_name]))\r\n",
        "            test_data[num_name] = test_data[num_name].replace(np.nan, np.mean(test_data[num_name]))\r\n",
        "    print('---str gogo---')\r\n",
        "    for str_name in str_list:\r\n",
        "        print('{} ok'.format(str_name))\r\n",
        "        if str_name in NANColumns:\r\n",
        "            train_data[str_name] = train_data[str_name].replace(np.nan, \"X\")\r\n",
        "            test_data[str_name] = test_data[str_name].replace(np.nan, \"X\")\r\n",
        "    print('---drop gogo---')\r\n",
        "    for drop_name in drop_list:\r\n",
        "        print('{} ok'.format(drop_name))\r\n",
        "        if drop_name in NANColumns:\r\n",
        "            train_data.drop(columns=[drop_name])\r\n",
        "            test_data.drop(columns=[drop_name])\r\n",
        "    '''\r\n",
        "    # 'pickup_datetime' split and turn type\r\n",
        "    df['pickup_datetime'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S UTC'))\r\n",
        "    df['pickup_year'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%Y')).astype('int64')\r\n",
        "    df['pickup_month'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%m')).astype('int64')\r\n",
        "    df['pickup_day'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%d')).astype('int64')\r\n",
        "    df['pickup_hour'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%H')).astype('int64')\r\n",
        "    df['pickup_minute'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%M')).astype('int64')\r\n",
        "    df['pickup_second'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%S')).astype('int64')\r\n",
        "\r\n",
        "    test_key = df['key'][100000:]\r\n",
        "    df = df.drop(['pickup_datetime'] , axis=1)\r\n",
        "    df = df.drop(['key'] , axis=1)\r\n",
        "\r\n",
        "    # Add some useful feather\r\n",
        "    df['longitude_diff'] = df['dropoff_longitude'] - df['pickup_longitude']\r\n",
        "    df['latitude_diff'] = df['dropoff_latitude'] - df['pickup_latitude']\r\n",
        "    df['distance_2D'] = (df['longitude_diff']**2 + df['latitude_diff']**2)**0.5\r\n",
        "    df['longitude_diff_difflat'] = (df['dropoff_longitude'] - df['pickup_longitude']) * np.cos((df['dropoff_latitude'] + df['pickup_latitude']) /2 /(360/2*np.pi))\r\n",
        "    df['latitude_diff_difflat'] = (df['dropoff_latitude'] - df['pickup_latitude']) * np.cos((df['dropoff_longitude'] + df['pickup_longitude']) /2 /(360/2*np.pi))\r\n",
        "\r\n",
        "    print(df.info)\r\n",
        "\r\n",
        "    # Scaler and df->train/test\r\n",
        "    scaler = MinMaxScaler()\r\n",
        "    df_scalered = scaler.fit_transform(df)\r\n",
        "    train_data, test_data = df_scalered[:100000], df_scalered[100000:]\r\n",
        "\r\n",
        "    # Split dataset and label, create complete/trainpart/crosspart data/label\r\n",
        "    split_num = 10000\r\n",
        "    all_num = 100000\r\n",
        "    train_data_complete = train_data\r\n",
        "    train_data_trainpart = train_data[0:split_num]\r\n",
        "    train_data_crosspart = train_data[split_num:all_num]\r\n",
        "    train_label_complete = train_label\r\n",
        "    train_label_trainpart = train_label[0:split_num]\r\n",
        "    train_label_crosspart = train_label[split_num:all_num]\r\n",
        "\r\n",
        "    '''\r\n",
        "    # Encoding, In order to distinguish numeric and categorical columns\r\n",
        "    CATEGORICAL_COLUMNS =[]\r\n",
        "    NUMERIC_COLUMNS =[]\r\n",
        "    i = 0\r\n",
        "    for a in train_data.dtypes:\r\n",
        "        if a == float or a == int:\r\n",
        "            NUMERIC_COLUMNS.append(train_data.columns[i])\r\n",
        "        elif a == object:\r\n",
        "            CATEGORICAL_COLUMNS.append(train_data.columns[i])\r\n",
        "        i += 1\r\n",
        "\r\n",
        "    le = LabelEncoder()\r\n",
        "    train_data_complete[CATEGORICAL_COLUMNS]    = train_data_complete[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\r\n",
        "    train_data_trainpart[CATEGORICAL_COLUMNS]   = train_data_trainpart[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\r\n",
        "    train_data_crosspart[CATEGORICAL_COLUMNS]   = train_data_crosspart[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col)) #SettingWithCopyWarning\r\n",
        "    test_data[CATEGORICAL_COLUMNS]              = test_data[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\r\n",
        "    '''\r\n",
        "\r\n",
        "    return train_data_complete, train_label_complete, train_data_trainpart, train_label_trainpart, train_data_crosspart, train_label_crosspart, test_data, test_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXHEYiJVsGog"
      },
      "source": [
        "# AI model fit, evaluate, predict store to csv\r\n",
        "def package_ai(train_data_complete, train_label_complete, train_data_crosspart, train_label_crosspart, test_data, test_key, model, model_name):\r\n",
        "    model.fit(train_data_complete, train_label_complete)\r\n",
        "    print(\"Result of {} is {}\".format(model_name, model.score(train_data_crosspart, train_label_crosspart)))\r\n",
        "\r\n",
        "    # Output\r\n",
        "    predicted_fare_amount_list = []\r\n",
        "    for predicted_fare_amount in model.predict(test_data):\r\n",
        "        predicted_fare_amount_list.append(float(predicted_fare_amount))\r\n",
        "\r\n",
        "    output = pd.DataFrame({'key':test_key, 'fare_amount': predicted_fare_amount_list})\r\n",
        "    output.to_csv(os.path.join(data_path, 'taxi', '{}_submissions.csv'.format(model_name)), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOvZ8OVCsI23"
      },
      "source": [
        "# tensorflow AI model create, compile, fit, evaluate, predict store to csv\r\n",
        "def tensorflow_ai(train_data_complete, train_label_complete, train_data_crosspart, train_label_crosspart, test_data, model_type):\r\n",
        "    model = tf.keras.Sequential()\r\n",
        "    if model_type == 'dnn':\r\n",
        "        model.add(tf.keras.layers.Flatten())\r\n",
        "        model.add(tf.keras.layers.Dense(\r\n",
        "                units = 1,\r\n",
        "                #input_shape = [1,74],\r\n",
        "                kernel_initializer = 'ones',\r\n",
        "                kernel_regularizer = tf.keras.regularizers.L1L2(l1=0, l2=1),\r\n",
        "            )\r\n",
        "        )\r\n",
        "        model.add(tf.keras.layers.Dense(50))\r\n",
        "        model.add(tf.keras.layers.Dense(50))\r\n",
        "        model.add(tf.keras.layers.Dense(50))\r\n",
        "        model.add(tf.keras.layers.Dense(25))\r\n",
        "        model.add(tf.keras.layers.Dense(10))\r\n",
        "        model.add(tf.keras.layers.Dense(1))\r\n",
        "    elif model_type == 'cnn':\r\n",
        "        model.add(tf.keras.layers.Flatten())\r\n",
        "        model.add(tf.keras.layers.Dense(50))\r\n",
        "        model.add(tf.keras.layers.Dense(25))\r\n",
        "        model.add(tf.keras.layers.Dense(10))\r\n",
        "        model.add(tf.keras.layers.Dense(1))\r\n",
        "\r\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n",
        "    model.compile(optimizer=opt, loss='mae')\r\n",
        "    train_history = model.fit(train_data_complete, train_label_complete, batch_size=8, epochs=20)\r\n",
        "\r\n",
        "    model.summary()\r\n",
        "    #acc = train_history.history['acc']\r\n",
        "    #val_acc = train_history.history['val_acc']\r\n",
        "    loss = train_history.history['loss']\r\n",
        "    #val_loss = train_history.history['val_loss']\r\n",
        "\r\n",
        "    epochs = range(1, len(loss)+1)\r\n",
        "    '''\r\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\r\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\r\n",
        "    plt.title('Training and validation accuracy')\r\n",
        "    plt.legend()\r\n",
        "    plt.figure()\r\n",
        "    '''\r\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\r\n",
        "    #plt.plot(epochs, val_loss, 'b', label='validation loss')\r\n",
        "    plt.title('Training loss')\r\n",
        "    plt.legend()\r\n",
        "    plt.savefig(os.path.join(data_path, 'taxi', '{} loss'.format(model_type)))\r\n",
        "    plt.close()\r\n",
        "\r\n",
        "    #print(\"Result of tensorflow_ai is {}\".format(model.score(train_data_crosspart, train_label_crosspart)))\r\n",
        "    print('tensorflow_ai evaluate:', model.evaluate(train_data_crosspart, train_label_crosspart))\r\n",
        "\r\n",
        "    # Output\r\n",
        "    predicted_fare_amount_list = []\r\n",
        "    for predicted_fare_amount in model.predict(test_data):\r\n",
        "        predicted_fare_amount_list.append(float(predicted_fare_amount))\r\n",
        "\r\n",
        "    output = pd.DataFrame({'key':test_key, 'fare_amount': predicted_fare_amount_list})\r\n",
        "    output.to_csv(os.path.join(data_path, 'taxi', 'tensorflow_ai_{}_submissions.csv'.format(model_type)), index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "S_jBYaDRsKM_",
        "outputId": "a072d17e-5ad7-4e7e-a755-4010b71bef3f"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    td, tl, tdt, tlt, tdc, tlc, test_data, test_key = taxi_data_deal()\r\n",
        "\r\n",
        "    # package_ai\r\n",
        "    ai_model_routing = {\r\n",
        "        #'mlp' : MLPRegressor(random_state=1, hidden_layer_sizes=(400,1), max_iter=400), #too large\r\n",
        "        'GReg' : GradientBoostingRegressor(random_state=0),\r\n",
        "        'CAT' : CatBoostRegressor(verbose=0, loss_function='RMSE'), #iterations=10, learning_rate=0.03, loss_function='MAE', n_estimators=300, verbose=0\r\n",
        "        'LGMB' : LGBMRegressor(),\r\n",
        "        'XGBRegressor' : XGBRegressor(objective='reg:squarederror'),\r\n",
        "        #'svr' : SVR(kernel='linear'), #too large\r\n",
        "        'lasso' : Lasso(),\r\n",
        "        'rf' : RandomForestRegressor(n_estimators=5, random_state=42),\r\n",
        "    }\r\n",
        "    for model_name, model in ai_model_routing.items():\r\n",
        "        print(model_name)\r\n",
        "        start = process_time()\r\n",
        "        package_ai(td, tl, tdc, tlc, test_data, test_key, model, model_name)\r\n",
        "        end = process_time()\r\n",
        "        print('{} spent time:'.format(model_name), end-start)\r\n",
        "        print()\r\n",
        "\r\n",
        "    # tensorflow_ai\r\n",
        "    start = process_time()\r\n",
        "    tensorflow_ai(td, tl, tdc, tlc, test_data, model_type='dnn')\r\n",
        "    end = process_time()\r\n",
        "    print('tensorflow_ai spent time:', end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method DataFrame.info of       pickup_longitude  ...  latitude_diff_difflat\n",
            "0           -73.844311  ...              -0.008964\n",
            "1           -74.016048  ...               0.070097\n",
            "2           -73.982738  ...              -0.010616\n",
            "3           -73.987130  ...               0.024736\n",
            "4           -73.968095  ...               0.015619\n",
            "...                ...  ...                    ...\n",
            "9909        -73.968124  ...              -0.016467\n",
            "9910        -73.945511  ...              -0.026997\n",
            "9911        -73.991600  ...              -0.078919\n",
            "9912        -73.985573  ...               0.065733\n",
            "9913        -73.988022  ...               0.005106\n",
            "\n",
            "[109914 rows x 16 columns]>\n",
            "GReg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f30992deb637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpackage_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtdc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} spent time:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2cc3c16c3413>\u001b[0m in \u001b[0;36mpackage_ai\u001b[0;34m(train_data_complete, train_label_complete, train_data_crosspart, train_label_crosspart, test_data, test_key, model, model_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# AI model fit, evaluate, predict store to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpackage_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_crosspart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_crosspart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_complete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Result of {} is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_crosspart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_crosspart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1245\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBl8R69ucK4"
      },
      "source": [
        "----------------------------------\r\n",
        "這邊用的模型有："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ipKiolluZA4"
      },
      "source": [
        "# package_ai\r\n",
        "ai_model_routing = {\r\n",
        "    #'mlp' : MLPRegressor(random_state=1, hidden_layer_sizes=(400,1), max_iter=400), #too large\r\n",
        "    'GReg' : GradientBoostingRegressor(random_state=0),\r\n",
        "    'CAT' : CatBoostRegressor(verbose=0, loss_function='RMSE'), #iterations=10, learning_rate=0.03, loss_function='MAE', n_estimators=300, verbose=0\r\n",
        "    'LGMB' : LGBMRegressor(),\r\n",
        "    'XGBRegressor' : XGBRegressor(objective='reg:squarederror'),\r\n",
        "    #'svr' : SVR(kernel='linear'), #too large\r\n",
        "    'lasso' : Lasso(),\r\n",
        "    'rf' : RandomForestRegressor(n_estimators=5, random_state=42),\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzCdE4eCuliA"
      },
      "source": [
        "最後是LGBMRegressor的結果最好\r\n",
        "\r\n",
        "(kaggle Score為3.50817，由於比賽時間已結束故手動對名次，為public第681名/1483參賽者)\r\n",
        "\r\n",
        "超參數：all default"
      ]
    }
  ]
}