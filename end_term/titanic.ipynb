{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EXKqIEctwfH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAlQYtJ_t0MX"
      },
      "source": [
        "# **分為import部分、資料處理部分、尋找最佳超參數部分、封裝AI模型部分、tensorflow dnn模型部分以及主程式**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se1awKa8t2ZW"
      },
      "source": [
        "詳細程式碼內容可看註解說明"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztjTg6zvt3I2"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "from time import process_time\r\n",
        "# Models\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.linear_model import Perceptron\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "# GridSearchCV\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "#Common Model Helpers\r\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "#from sklearn import feature_selection\r\n",
        "#from sklearn import model_selection\r\n",
        "\r\n",
        "data_path = '/content/drive/My Drive/天氣AI/end_term/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztguaf_4t7Z2"
      },
      "source": [
        "# Deal with titanic_data\r\n",
        "def titanic_data_deal():\r\n",
        "    train_num = 891\r\n",
        "    # df create\r\n",
        "    try:\r\n",
        "        test_df = pd.read_csv(os.path.join(os.getcwd(), 'test.csv'))\r\n",
        "        train_df = pd.read_csv(os.path.join(os.getcwd(), 'train.csv'))\r\n",
        "    except FileNotFoundError:\r\n",
        "        test_df = pd.read_csv(os.path.join(os.getcwd(), 'titanic', 'test.csv'))\r\n",
        "        train_df = pd.read_csv(os.path.join(os.getcwd(), 'titanic', 'train.csv'))\r\n",
        "    data_df = train_df.append(test_df)\r\n",
        "\r\n",
        "    # Engineering features\r\n",
        "    '''\r\n",
        "    # Cleaning name and extracting Title\r\n",
        "    for name_string in data_df['Name']:\r\n",
        "        data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\r\n",
        "\r\n",
        "    # Replacing rare titles with more common ones\r\n",
        "    mapping = { 'Mlle': 'Miss',\r\n",
        "                'Major': 'Mr',\r\n",
        "                'Col': 'Mr',\r\n",
        "                'Sir': 'Mr',\r\n",
        "                'Don': 'Mr',\r\n",
        "                'Mme': 'Miss',\r\n",
        "                'Jonkheer': 'Mr',\r\n",
        "                'Lady': 'Mrs',\r\n",
        "                'Capt': 'Mr',\r\n",
        "                'Countess':'Mrs',\r\n",
        "                'Ms': 'Miss',\r\n",
        "                'Dona': 'Mrs'\r\n",
        "            }\r\n",
        "    data_df.replace({'Title': mapping}, inplace=True)\r\n",
        "    titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\r\n",
        "    for title in titles:\r\n",
        "        age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\r\n",
        "        data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\r\n",
        "    '''\r\n",
        "\r\n",
        "    # Substituting Age values in TRAIN_DF and TEST_DF:\r\n",
        "    train_df['Age'] = data_df['Age'][:train_num]\r\n",
        "    test_df['Age'] = data_df['Age'][train_num:]\r\n",
        "    # Dropping Title feature\r\n",
        "    #data_df.drop('Title', axis = 1, inplace = True)\r\n",
        "    # Adding Family_Size\r\n",
        "    data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\r\n",
        "    train_df['Family_Size'] = data_df['Family_Size'][:train_num]\r\n",
        "    test_df['Family_Size'] = data_df['Family_Size'][train_num:]\r\n",
        "    # Adding Family_Survival\r\n",
        "    data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\r\n",
        "    data_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\r\n",
        "    DEFAULT_SURVIVAL_VALUE = 0.5\r\n",
        "    data_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\r\n",
        "\r\n",
        "    # Using data_df['Last_Name', 'Fare'] to find 'Family_Survival'\r\n",
        "    for _, grp_df in data_df[['Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\r\n",
        "                            'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\r\n",
        "        if (len(grp_df) != 1):\r\n",
        "            # A Family group is found.\r\n",
        "            for ind, row in grp_df.iterrows():\r\n",
        "                smax = grp_df.drop(ind)['Survived'].max()\r\n",
        "                smin = grp_df.drop(ind)['Survived'].min()\r\n",
        "                passID = row['PassengerId']\r\n",
        "                if (smax == 1.0):\r\n",
        "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\r\n",
        "                elif (smin==0.0):\r\n",
        "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\r\n",
        "    print(\"Number of passengers with family survival information:\",\r\n",
        "        data_df.loc[data_df['Family_Survival']!=0.5].shape[0])\r\n",
        "\r\n",
        "    # Using data_df['Ticket'] to find 'Family_Survival'\r\n",
        "    for _, grp_df in data_df.groupby('Ticket'):\r\n",
        "        if (len(grp_df) != 1):\r\n",
        "            for ind, row in grp_df.iterrows():\r\n",
        "                if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\r\n",
        "                    smax = grp_df.drop(ind)['Survived'].max()\r\n",
        "                    smin = grp_df.drop(ind)['Survived'].min()\r\n",
        "                    passID = row['PassengerId']\r\n",
        "                    if (smax == 1.0):\r\n",
        "                        data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\r\n",
        "                    elif (smin==0.0):\r\n",
        "                        data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\r\n",
        "    print(\"Number of passenger with family/group survival information: \"\r\n",
        "        +str(data_df[data_df['Family_Survival']!=0.5].shape[0]))\r\n",
        "\r\n",
        "    train_df['Family_Survival'] = data_df['Family_Survival'][:train_num]\r\n",
        "    test_df['Family_Survival'] = data_df['Family_Survival'][train_num:]\r\n",
        "\r\n",
        "    # Making FARE BINS\r\n",
        "    data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\r\n",
        "    # Making Bins\r\n",
        "    data_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\r\n",
        "    label = LabelEncoder()\r\n",
        "    data_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\r\n",
        "    train_df['FareBin_Code'] = data_df['FareBin_Code'][:train_num]\r\n",
        "    test_df['FareBin_Code'] = data_df['FareBin_Code'][train_num:]\r\n",
        "    train_df.drop(['Fare'], 1, inplace=True)\r\n",
        "    test_df.drop(['Fare'], 1, inplace=True)\r\n",
        "    # Making AGE BINS\r\n",
        "    data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\r\n",
        "    label = LabelEncoder()\r\n",
        "    data_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\r\n",
        "    train_df['AgeBin_Code'] = data_df['AgeBin_Code'][:train_num]\r\n",
        "    test_df['AgeBin_Code'] = data_df['AgeBin_Code'][train_num:]\r\n",
        "    train_df.drop(['Age'], 1, inplace=True)\r\n",
        "    test_df.drop(['Age'], 1, inplace=True)\r\n",
        "    # Mapping SEX and cleaning data (dropping garbage)\r\n",
        "    train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\r\n",
        "    test_df['Sex'].replace(['male','female'],[0,1],inplace=True)\r\n",
        "    #train_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\r\n",
        "    #            'Embarked'], axis = 1, inplace = True)\r\n",
        "    #test_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\r\n",
        "     #           'Embarked'], axis = 1, inplace = True)\r\n",
        "    train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin', 'Embarked', ], axis = 1, inplace = True)\r\n",
        "    test_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin', 'Embarked', ], axis = 1, inplace = True)\r\n",
        "\r\n",
        "    print(train_df.info)\r\n",
        "    print(test_df.info)\r\n",
        "    return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz2cZNvCuBow"
      },
      "source": [
        "# Using GridSearchCV to find hyperparams\r\n",
        "def knn_find_para(X, y, X_test):\r\n",
        "    # Grid Search CV\r\n",
        "    algorithm = ['auto']\r\n",
        "    weights = ['uniform', 'distance']\r\n",
        "    leaf_size = list(range(1, 50, 5))\r\n",
        "    n_neighbors = [6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22]\r\n",
        "    hyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, 'n_neighbors': n_neighbors}\r\n",
        "    gd = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=hyperparams, verbose=True, cv=10, scoring='roc_auc')\r\n",
        "    gd.fit(X, y)\r\n",
        "    print(gd.best_score_)\r\n",
        "    print(gd.best_estimator_)\r\n",
        "    #gd.best_estimator_.fit(X, y)\r\n",
        "    #y_pred = gd.best_estimator_.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkQ6EM4uCIn"
      },
      "source": [
        "# AI model fit, evaluate, predict store to csv\r\n",
        "def package_ai(train_data_complete, train_label_complete, test_data, model, model_name):\r\n",
        "    model.fit(train_data_complete, train_label_complete)\r\n",
        "    print(\"Result of {} is {}\".format(model_name, model.score(train_data_complete, train_label_complete)))\r\n",
        "\r\n",
        "    # Output\r\n",
        "    predicted_survived_list = []\r\n",
        "    for predicted_survived in model.predict(test_data):\r\n",
        "        predicted_survived_list.append(int(predicted_survived))\r\n",
        "\r\n",
        "    output = pd.DataFrame({'PassengerId':range(892, 1309+1), 'Survived': predicted_survived_list})\r\n",
        "    output.to_csv(os.path.join(os.getcwd(), 'titanic', '{}_submissions.csv'.format(model_name)), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYLFKvMEuE00"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    # Data deal and split\r\n",
        "    train_df, test_df = titanic_data_deal()\r\n",
        "    X = train_df.drop('Survived', 1)\r\n",
        "    y = train_df['Survived']\r\n",
        "    X_test = test_df.copy()\r\n",
        "\r\n",
        "    std_scaler = StandardScaler()\r\n",
        "    X = std_scaler.fit_transform(X)\r\n",
        "    X_test = std_scaler.transform(X_test)\r\n",
        "\r\n",
        "    # knn algo\r\n",
        "    #knn_find_para(X, y, X_test)\r\n",
        "\r\n",
        "    # package_ai\r\n",
        "    ai_model_routing = {\r\n",
        "        'SVC' : SVC(),\r\n",
        "        'logist' : LogisticRegression(),\r\n",
        "        'perceptron' : Perceptron(),\r\n",
        "        'gaussian' : GaussianNB(),\r\n",
        "        'decision_tree' : DecisionTreeClassifier(criterion='entropy'),\r\n",
        "        'adaboost' : AdaBoostClassifier(),\r\n",
        "        'MLP' : MLPClassifier(),\r\n",
        "        'lasso' : Lasso(),\r\n",
        "        'rf' : RandomForestClassifier(n_estimators=5, random_state=42, criterion='entropy'), # n_estimators=100/5\r\n",
        "        'knn' : KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski',\r\n",
        "                            metric_params=None, n_jobs=1, n_neighbors=6, p=2, weights='uniform'),\r\n",
        "    }\r\n",
        "    for model_name, model in ai_model_routing.items():\r\n",
        "        start = process_time()\r\n",
        "        package_ai(X, y, X_test, model, model_name)\r\n",
        "        end = process_time()\r\n",
        "        print('{} spent time:'.format(model_name), end-start)\r\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU1V6k_gu_iQ"
      },
      "source": [
        "----------------------------------\r\n",
        "這邊用的模型有："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSWjvaCWu_p_"
      },
      "source": [
        "    # package_ai\r\n",
        "    ai_model_routing = {\r\n",
        "        'SVC' : SVC(),\r\n",
        "        'logist' : LogisticRegression(),\r\n",
        "        'perceptron' : Perceptron(),\r\n",
        "        'gaussian' : GaussianNB(),\r\n",
        "        'decision_tree' : DecisionTreeClassifier(criterion='entropy'),\r\n",
        "        'adaboost' : AdaBoostClassifier(),\r\n",
        "        'MLP' : MLPClassifier(),\r\n",
        "        'lasso' : Lasso(),\r\n",
        "        'rf' : RandomForestClassifier(n_estimators=5, random_state=42, criterion='entropy'), # n_estimators=100/5\r\n",
        "        'knn' : KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski',\r\n",
        "                            metric_params=None, n_jobs=1, n_neighbors=6, p=2, weights='uniform'),\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZ4Hrjqu_xu"
      },
      "source": [
        "最後是KNeighborsClassifier的結果最好\r\n",
        "\r\n",
        "(kaggle Score為0.81818，為public第467名/20352參賽者)\r\n",
        "\r\n",
        "超參數：algorithm='auto', leaf_size=26, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=6, p=2, weights='uniform'"
      ]
    }
  ]
}